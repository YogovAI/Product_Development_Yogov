I have designed the design and completed the loading of data from flat files to database table. pls correct or re-design the approach. 
1. created data sources section, which will create lot of external data sources like Flat Files, RDBMS, No sql, API, Websites, Datalake, website scrap etc ... 
2.created Extractor Service, which fetches data sources which we created earlier and get source level column details, data types, total number of record from source and data memory size etc ... 3.Created Transformer Templates, here create target system which might be RDBMS, data lake, api etc... for demo purpose we kept source as flat files and target as postgres db. we kept load type as single table, multi table, adhoc load and api load. if it is single table load means we fetch table name to load and if it is adhoc load means we will get new table name and create new table and load the data. then from extractor service we fetch schema and create column rule into it. and in business we kept plain english to define rule. 
4. we created mapper for source schema we fetch from extractor service and target schema w fetch from transformer template. and we map columns from source to target. 
5.Finally we created loader service which will fetch mapper service and loads data into target table. pls suggest any design changes in this etl tool.
